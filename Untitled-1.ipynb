{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdf839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\program files\\python310\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python310\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\program files\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\program files\\python310\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python310\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\program files\\python310\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\program files\\python310\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python310\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\program files\\python310\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\program files\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\program files\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python310\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python310\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\program files\\python310\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\program files\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\program files\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python310\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\program files\\python310\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python310\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\program files\\python310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python310\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\donmi\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\program files\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\program files\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\program files\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\program files\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\program files\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donmi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset shape: (50000, 2)\n",
      "Columns: ['review', 'sentiment']\n",
      "First few rows:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset from the CSV file\n",
    "#Code Explanation\n",
    "#This code snippet loads the IMDB dataset from a CSV file into a pandas DataFrame and displays its shape, columns, and the first few rows.\n",
    "#-Import necessary libraries\n",
    "#- Load the IMDB dataset from a CSV file into a DataFrame\n",
    "#- Print a success message\n",
    "#- Print the shape of the DataFrame\n",
    "#- Print the column names of the DataFrame\n",
    "#- Print the first few rows of the DataFrame\"\n",
    "%pip install transformers datasets torch scikit-learn matplotlib seaborn tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('imdb.csv')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\\n",
    "First few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cce566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using subset of 10000 samples\n",
      "Label distribution:\n",
      "labels\n",
      "1    5039\n",
      "0    4961\n",
      "Name: count, dtype: int64\n",
      "Training samples: 8000\n",
      "Test samples: 2000\n"
     ]
    }
   ],
   "source": [
    "#Code Explanation The code snippet cleans text data, converts sentiment labels to numeric values, and splits the dataset into training and testing subsets.\n",
    "#- Define a function to clean text by removing HTML tags and extra whitespace.\n",
    "#- Apply the cleaning function to the review column of the DataFrame.\n",
    "#- Map sentiment labels to numeric values (1 for positive, 0 for negative).\n",
    "#- Sample a subset of 10,000 reviews for faster training.\n",
    "#- Split the subset into training and testing sets while maintaining label distribution.\n",
    "# Clean and prepare the data\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean HTML tags and extra whitespace from text\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Clean the reviews\n",
    "df['review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Convert sentiment labels to numeric\n",
    "df['labels'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Take a smaller subset for faster training (10,000 samples)\n",
    "df_subset = df.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Using subset of {len(df_subset)} samples\")\n",
    "print(f\"Label distribution:\")\n",
    "print(df_subset['labels'].value_counts())\n",
    "\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_subset['review'].tolist(),\n",
    "    df_subset['labels'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_subset['labels']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0833483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased\n",
      "Model parameters: 66,955,010\n",
      "Tokenizing training data...\n",
      "Tokenizing test data...\n",
      "Training input shape: torch.Size([8000, 256])\n",
      "Test input shape: torch.Size([2000, 256])\n"
     ]
    }
   ],
   "source": [
    "#Code Explanation\n",
    "#The code snippet initializes a DistilBERT model and tokenizer for binary sentiment classification and tokenizes the training and test datasets.\n",
    "#- Set model name to 'distilbert-base-uncased'.\n",
    "#- Load tokenizer and model for sequence classification.\n",
    "#- Define a function to tokenize text data with specified maximum length.\n",
    "#- Tokenize training and test data, storing input IDs, attention masks, and labels.\n",
    "#- Print shapes of tokenized training and test inputs.\n",
    "\n",
    "import os\n",
    "\n",
    "# Unset CA bundle environment variables if set (fixes TLS CA certificate bundle error)\n",
    "os.environ.pop(\"REQUESTS_CA_BUNDLE\", None)\n",
    "os.environ.pop(\"CURL_CA_BUNDLE\", None)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1}\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(texts, labels, max_length=256):\n",
    "    \"\"\"Tokenize text data\"\"\"\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "print(\"Tokenizing training data...\")\n",
    "train_encodings = tokenize_data(train_texts, train_labels)\n",
    "print(\"Tokenizing test data...\")\n",
    "test_encodings = tokenize_data(test_texts, test_labels)\n",
    "\n",
    "print(f\"Training input shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Test input shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f85af559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 8000\n",
      "Test dataset size: 2000\n",
      "Data loaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "#This code defines a custom dataset for the IMDB movie reviews and creates data loaders for training and testing.\n",
    "#- Defined IMDBDataset class inheriting from torch.utils.data.Dataset\n",
    "#- Implemented __getitem__ and __len__ methods for dataset functionality\n",
    "#- Created train and test datasets using IMDBDataset\n",
    "#- Initialized data loaders for both datasets with specified batch sizes and shuffling options\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IMDBDataset(train_encodings)\n",
    "test_dataset = IMDBDataset(test_encodings)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fe2101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [25:09<00:00, 12.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Performance (before fine-tuning):\n",
      "Accuracy: 0.5040\n",
      "F1 Score: 0.6702\n",
      "Average Loss: 0.6946\n"
     ]
    }
   ],
   "source": [
    "#This code snippet evaluates the performance of a model on a test dataset before fine-tuning by calculating accuracy, F1 score, and average loss.\n",
    "# Set model to evaluation mode\n",
    "#- Initialize lists for predictions and true labels\n",
    "#- Loop through data loader batches\n",
    "#- Get model outputs and calculate loss\n",
    "#- Store predictions and true labels\n",
    "#- Calculate accuracy and F1 score\n",
    "#- Return accuracy, F1 score, and average loss\n",
    "\n",
    "# Evaluate baseline model performance before fine-tuning\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return accuracy, f1, avg_loss\n",
    "\n",
    "print(\"Evaluating baseline model performance...\")\n",
    "baseline_accuracy, baseline_f1, baseline_loss = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f\"\\\n",
    "Baseline Performance (before fine-tuning):\")\n",
    "print(f\"Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"F1 Score: {baseline_f1:.4f}\")\n",
    "print(f\"Average Loss: {baseline_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168ff6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 500/500 [2:10:28<00:00, 15.66s/it, loss=0.2005]   \n",
      "Evaluating: 100%|██████████| 125/125 [06:15<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:\n",
      "  Train Loss: 0.3323\n",
      "  Test Loss: 0.2741\n",
      "  Test Accuracy: 0.8845\n",
      "  Test F1: 0.8804\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 500/500 [1:17:53<00:00,  9.35s/it, loss=0.1293]\n",
      "Evaluating: 100%|██████████| 125/125 [06:18<00:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:\n",
      "  Train Loss: 0.1742\n",
      "  Test Loss: 0.2803\n",
      "  Test Accuracy: 0.8865\n",
      "  Test F1: 0.8930\n",
      "--------------------------------------------------\n",
      "Fine-tuning completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#This code snippet implements a manual training loop to fine-tune a model using a specified number of epochs and learning rate.\n",
    "#- Defined a function to train the model with training and evaluation phases.\n",
    "#- Set up an optimizer with weight decay.\n",
    "#- Iterated over epochs to train the model, calculating and storing training loss.\n",
    "#- Evaluated the model after each epoch, calculating accuracy and F1 score.\n",
    "#- Printed training and evaluation metrics for each epoch.\n",
    "\n",
    "# Fine-tune the model\n",
    "def train_model(model, train_loader, test_loader, epochs=2, learning_rate=2e-5):\n",
    "    \"\"\"Train the model with manual training loop\"\"\"\n",
    "    \n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    test_f1_scores = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        test_accuracy, test_f1, test_loss = evaluate_model(model, test_loader)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        test_f1_scores.append(test_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'  Test Loss: {test_loss:.4f}')\n",
    "        print(f'  Test Accuracy: {test_accuracy:.4f}')\n",
    "        print(f'  Test F1: {test_f1:.4f}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return train_losses, test_accuracies, test_f1_scores\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "train_losses, test_accuracies, test_f1_scores = train_model(model, train_loader, test_loader, epochs=2)\n",
    "\n",
    "print(\"\\\n",
    "Fine-tuning completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
